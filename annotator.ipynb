{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import time\n",
    "import tqdm\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm.notebook as tq\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "from torch.nn import ConvTranspose2d\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import Module\n",
    "from torch.nn import ModuleList\n",
    "from torch.nn import ReLU\n",
    "from torchvision.transforms import CenterCrop\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torchmetrics import JaccardIndex\n",
    "import threading\n",
    "\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "from semseg import show_models\n",
    "from semseg.models import *\n",
    "\n",
    "from whuvid_dataset import WhuvidDataset\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "sys.path.append('./sort') # scikit-image filterpy\n",
    "import sort\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# from deep_sort_realtime.deepsort_tracker import DeepSort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    # transforms.RandomRotation(10),\n",
    "    # transforms.RandomResizedCrop(size=224, scale=(0.4, 1.0)\n",
    "    #                             #  , ratio=(0.5, 2)\n",
    "    #                              ),\n",
    "    # resize to 224x224\n",
    "    #transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence has 6470 images\n"
     ]
    }
   ],
   "source": [
    "sequence = \"31\"\n",
    "whuvid_base_path = \"/home/thiago/Workspace/motion-segmentation/datasets/WHUVID\"\n",
    "whuvid_dataset = WhuvidDataset(whuvid_base_path, [sequence], transform, segmentation=False, flow=False, use_gdino=False)\n",
    "\n",
    "# random_index = random.randint(0, len(whuvid_dataset))\n",
    "\n",
    "# img, flow, imu, mask = whuvid_dataset[random_index]\n",
    "# height, width = img.shape[1:]\n",
    "# # plot\n",
    "# fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "# axes[0].imshow(np.transpose(img, (1, 2, 0)))\n",
    "# axes[1].imshow(mask[0], cmap='gray') \n",
    "# axes[2].imshow(np.transpose(flow, (1, 2, 0)))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6470"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(whuvid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boxmot import DeepOCSORT, StrongSORT, BoTSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-01 19:22:58.449\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mboxmot.utils.torch_utils\u001b[0m:\u001b[36mselect_device\u001b[0m:\u001b[36m52\u001b[0m - \u001b[1mYolo Tracking v10.0.83 ðŸš€ Python-3.12.7 torch-2.2.2+cu121\n",
      "CUDA:0 (NVIDIA GeForce RTX 4080, 15952MiB)\u001b[0m\n",
      "\u001b[32m2024-11-01 19:22:58.490\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mboxmot.appearance.reid_model_factory\u001b[0m:\u001b[36mload_pretrained_weights\u001b[0m:\u001b[36m183\u001b[0m - \u001b[32m\u001b[1mLoaded pretrained weights from osnet_x0_25_msmt17.pt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tracker = BoTSORT(\n",
    "    model_weights=Path('osnet_x0_25_msmt17.pt'), # which ReID model to use\n",
    "    device='cuda:0',\n",
    "    fp16=False,\n",
    "    new_track_thresh=0.3,\n",
    "    track_high_thresh=0.4,\n",
    "    track_low_thresh=0.1,\n",
    "    match_thresh=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 223 objects\n"
     ]
    }
   ],
   "source": [
    "vit_path = \"/home/thiago/Workspace/motion-segmentation/src/ViT-B-32.pt\"\n",
    "# tracker = DeepSort(max_age=5, embedder_gpu=True, embedder='clip_ViT-B/32')\n",
    "# tracker = DeepSort(max_age=100, embedder='clip_ViT-B/32', embedder_wts=vit_path, embedder_gpu=True)\n",
    "\n",
    "obj_path = os.path.join(whuvid_base_path, sequence, \"other_files\", \"objects.json\")\n",
    "if os.path.exists(obj_path):\n",
    "    with open(obj_path, 'r') as f:\n",
    "        objects = json.load(f)\n",
    "    # convert keys to int\n",
    "    objects = {int(k): \n",
    "                {int(k2): v2 for k2, v2 in v.items() if v2 is not None}\n",
    "                for k, v in objects.items()}\n",
    "    print(f\"Loaded {len(objects)} objects\")\n",
    "else:\n",
    "    # objects = defaultdict(lambda: defaultdict(lambda: [None, None]))\n",
    "    objects = defaultdict(lambda: {})\n",
    "\n",
    "    # run full sort inference\n",
    "    for i in tq.tqdm(range(len(whuvid_dataset))):\n",
    "    # for i in tq.tqdm(range(500)):\n",
    "        img_path = whuvid_dataset.images[i]\n",
    "        img = cv2.imread(img_path)\n",
    "        bbs = whuvid_dataset.bounding_boxes[i]\n",
    "\n",
    "        # print(f\"bbs: {bbs}\")\n",
    "\n",
    "        bbs_deepsort = []\n",
    "        for bb in bbs:\n",
    "            x1, y1, x2, y2, confidence, name = bb\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "            # to ((left,top,w,h), confidence, name)\n",
    "            # bbs_deepsort.append([(x1, y1, x2-x1, y2-y1), confidence, name]) \n",
    "            bbs_deepsort.append([x1, y1, x2, y2, confidence, 0]) \n",
    "\n",
    "        if len(bbs) == 0:\n",
    "            # tracked = tracker.update_tracks(bbs_deepsort, frame=img)\n",
    "            # tracked = tracker.update(np.array(bbs_deepsort),img)\n",
    "            tracked = tracker.update(np.empty((0, 6)),img)\n",
    "        else:\n",
    "            # tracked = tracker.update_tracks(bbs_deepsort, frame=img)\n",
    "            tracked = tracker.update(np.array(bbs_deepsort),img)\n",
    "\n",
    "        for t in tracked:\n",
    "            # if not t.is_confirmed():\n",
    "            #     continue\n",
    "            # print(t)\n",
    "            # id = t.track_id\n",
    "            # # ltrb = t.to_ltrb(orig=True, orig_strict=True)\n",
    "            # ltrb = t.to_ltrb(orig=False)\n",
    "            # if ltrb is not None:\n",
    "            #     bb = ltrb.astype(int).tolist()\n",
    "            #     objects[id][i] = [bb, None]\n",
    "            x1, y1, x2, y2, id, _, _, _ = t\n",
    "            bb = [x1, y1, x2, y2]\n",
    "            objects[int(id)][i] = [bb, None]\n",
    "    print(f\"Tracked {len(objects)} objects\")\n",
    "    with open(obj_path, 'w') as f:\n",
    "        json.dump(objects, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start at false for every object\n",
    "delete_object = {k: False for k in objects.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "004f9a32398445f286ccdf0e2d1b7f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6470 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_click = False\n",
    "# click_is_left = False\n",
    "click_button = \"left\"\n",
    "def get_mouse_clicks(event, x, y, flags, param):\n",
    "    global mouseX, mouseY, new_click, click_button\n",
    "    # if left or middle\n",
    "    if event == cv2.EVENT_LBUTTONDOWN or event == cv2.EVENT_RBUTTONDOWN or event == cv2.EVENT_MBUTTONDOWN:\n",
    "        mouseX, mouseY = x, y\n",
    "        new_click = True\n",
    "    # click_is_left = event == cv2.EVENT_LBUTTONDOWN\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        click_button = \"left\"\n",
    "    elif event == cv2.EVENT_RBUTTONDOWN:\n",
    "        click_button = \"right\"\n",
    "    elif event == cv2.EVENT_MBUTTONDOWN:\n",
    "        click_button = \"middle\"\n",
    "\n",
    "def clicks_inside_bb(x, y, tracked_bbs):\n",
    "    bb_clicks = []\n",
    "    for id, bb, state in tracked_bbs:\n",
    "        if bb is None:\n",
    "            continue\n",
    "        x1, y1, x2, y2 = [int(v) for v in bb]\n",
    "        if x1 <= x <= x2 and y1 <= y <= y2:\n",
    "            bb_clicks.append(id)\n",
    "    return bb_clicks\n",
    "\n",
    "# find most recent frame that has a state\n",
    "# same state as that. If never has state, return False\n",
    "def is_in_motion(id, i, objects):\n",
    "    for j in range(i, -1, -1):\n",
    "        if j in objects[id]:\n",
    "            state = objects[id][j][1]\n",
    "            if state is not None:\n",
    "                return state\n",
    "    return False\n",
    "\n",
    "# ex: 0-True, 1-True, 2-None, 3-None, return 2 for i == 4 or 3 or 2\n",
    "def find_none_after_state(id, i, objects):\n",
    "    first_frame = min(objects[id].keys())\n",
    "    for j in range(i, first_frame - 1, -1):\n",
    "        if j in objects[id]:\n",
    "            state = objects[id][j][1] if j in objects[id] else None\n",
    "            previous_state = objects[id][j-1][1] if j-1 in objects[id] else None\n",
    "            if state is None and (j == first_frame or previous_state is not None):\n",
    "                return j\n",
    "    return first_frame\n",
    "\n",
    "cv2.namedWindow('image')\n",
    "cv2.setMouseCallback('image', get_mouse_clicks)\n",
    "\n",
    "# traked_objects = defaultdict(defaultdict(bool))\n",
    "# objects_in_motion = set()\n",
    "\n",
    "play_video = True\n",
    "i = 0\n",
    "# show dataset as opencv video\n",
    "bar = tq.tqdm(total=len(whuvid_dataset))\n",
    "while True:\n",
    "    # set bar to i\n",
    "    bar.n = i\n",
    "    bar.last_print_n = i\n",
    "    bar.update() \n",
    "    img_path = whuvid_dataset.images[i]\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    if img is None:\n",
    "        print(f\"None image for frame {i}\")\n",
    "        i = min(len(whuvid_dataset), i + 1)\n",
    "        continue\n",
    "\n",
    "    tracked_bbs = [[id, data[i][0], data[i][1]] for id, data in objects.items() if i in data]\n",
    "\n",
    "    if new_click:\n",
    "        bb_selected = clicks_inside_bb(mouseX, mouseY, tracked_bbs)\n",
    "        for id in bb_selected:\n",
    "            if click_button == \"middle\":\n",
    "                delete_object[id] = not delete_object[id]\n",
    "            else:\n",
    "                frame_to_set = find_none_after_state(id, i, objects) if (click_button == \"left\") else i\n",
    "                # state_of_frame = objects[id][frame_to_set][1]\n",
    "                # if state_of_frame is None or state is False:\n",
    "                #     objects[id][frame_to_set][1] = True\n",
    "                # else:\n",
    "                #     objects[id][frame_to_set][1] = False\n",
    "                state = is_in_motion(id, i, objects)\n",
    "                # print(f\"Setting id {id} to not {state} in frame {frame_to_set}\")\n",
    "                objects[id][frame_to_set][1] = not state\n",
    "        new_click = False\n",
    "\n",
    "    # draw\n",
    "    for id, bb, state in tracked_bbs:\n",
    "        if bb is None:\n",
    "            print(f\"None bb for id {id} in frame {i}\")\n",
    "        x1, y1, x2, y2 = [int(v) for v in bb]\n",
    "        if delete_object[id]:\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "        else:\n",
    "            annotated_state = objects[id][i][1]\n",
    "            if annotated_state is True:\n",
    "                # write an asterisk\n",
    "                cv2.putText(img, \"*\", (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "            infered_state = is_in_motion(id, i, objects)\n",
    "            center_point = (int((x1 + x2) / 2), int((y1 + y2) / 2))\n",
    "            if infered_state is True:\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "                # write object id in yellow\n",
    "                # cv2.putText(img, str(id), center_point, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "            else:\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                # write object id in yellow\n",
    "                # cv2.putText(img, str(id), center_point, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "    # show\n",
    "    cv2.imshow('image', img)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    # if key != 255:\n",
    "    #     print(f\"key: {key}\")\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    elif key == ord(' '):\n",
    "        play_video = not play_video\n",
    "    # elif key == 81:\n",
    "    elif key == ord('a'):\n",
    "        i = max(0, i - 1)\n",
    "        img = cv2.imread(whuvid_dataset.images[i])\n",
    "        if img is None:\n",
    "            i = max(0, i - 1)\n",
    "    # elif key == 83:\n",
    "    elif key == ord('d'):\n",
    "        i = min(len(whuvid_dataset), i + 1)\n",
    "        img = cv2.imread(whuvid_dataset.images[i])\n",
    "        if img is None:\n",
    "            i = min(len(whuvid_dataset), i + 1)\n",
    "\n",
    "    if play_video:\n",
    "        i = min(len(whuvid_dataset), i + 1)\n",
    "    else:\n",
    "        pass\n",
    "# close\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use is_in_motion to infer state of all frames\n",
    "obj_inferred = objects.copy()\n",
    "for id, data in objects.items():\n",
    "    if delete_object[id]:\n",
    "        continue\n",
    "    for i in range(len(whuvid_dataset)):\n",
    "        if i not in data:\n",
    "            continue\n",
    "        state = is_in_motion(id, i, objects)\n",
    "        obj_inferred[id][i][1] = state\n",
    "# invert everything\n",
    "# obj_inferred = {id: {i: [bb, not state] for i, (bb, state) in data.items()} for id, data in obj_inferred.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_path_inf = os.path.join(whuvid_base_path, sequence, \"other_files\", \"objects_inferred.json\")\n",
    "with open(obj_path_inf, 'w') as f:\n",
    "    json.dump(obj_inferred, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
