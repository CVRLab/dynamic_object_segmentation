{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import time\n",
    "import tqdm\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm.notebook as tq\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "from torch.nn import ConvTranspose2d\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import Module\n",
    "from torch.nn import ModuleList\n",
    "from torch.nn import ReLU\n",
    "from torchvision.transforms import CenterCrop\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import threading\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "from semseg import show_models\n",
    "from semseg.models import *\n",
    "import ptlflow\n",
    "from ptlflow.utils import flow_utils\n",
    "from ptlflow.utils.io_adapter import IOAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width = 512, 512\n",
    "# height, width = 256, 256\n",
    "with_augs = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # transforms.RandomRotation(10),\n",
    "    transforms.RandomResizedCrop(size=(height, width), scale=(0.2, 1.0), ratio=(0.5, 2)),\n",
    "    # transforms.Resize((height, width)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    # transforms.RandomRotation(10),\n",
    "    # transforms.RandomResizedCrop(size=224, scale=(0.4, 1.0)\n",
    "    #                             #  , ratio=(0.5, 2)\n",
    "    #                              ),\n",
    "    # resize to 224x224\n",
    "    # transforms.Resize((512, 512)),\n",
    "    transforms.Resize((height, width)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_classes = 3\n",
    "\n",
    "def get_binary_masks(pred):\n",
    "    predicted_classes = np.argmax(pred, axis=0)\n",
    "    masks = []\n",
    "    for i in range(num_classes):\n",
    "        mask = (predicted_classes == i).astype(np.uint8)\n",
    "        masks.append(mask)\n",
    "    return masks\n",
    "\n",
    "def to_painted_image(image, label):\n",
    "    green_image = np.zeros_like(image)\n",
    "    blue_image = np.zeros_like(image)\n",
    "    green_image[:, :] = [0, 255, 0]  # Green color in BGR\n",
    "    blue_image[:, :] = [0, 0, 255]   # Blue color in BGR\n",
    "\n",
    "    masks = get_binary_masks(label)\n",
    "\n",
    "    alpha = 0.998  # Alpha value for blending\n",
    "    beta = 1.0 - alpha\n",
    "    blended_green = cv2.addWeighted(image, alpha, green_image, beta, 0)\n",
    "    blended_blue = cv2.addWeighted(image, alpha, blue_image, beta, 0)\n",
    "    blended_green = np.clip(blended_green, 0, 1)\n",
    "    blended_blue = np.clip(blended_blue, 0, 1)\n",
    "\n",
    "    output_image = image.copy()\n",
    "    output_image[masks[0] == 1] = blended_blue[masks[0] == 1]\n",
    "    output_image[masks[1] == 1] = blended_green[masks[1] == 1]\n",
    "\n",
    "    return output_image\n",
    "\n",
    "def print_dataset(dataset, i):\n",
    "    img, flow, imu, mask = dataset[i]\n",
    "\n",
    "    # flow = flow.to(device)\n",
    "    # print(flow.shape)\n",
    "    # imu = imu.to(device)\n",
    "    # imu_x = model.imu_dense(imu.view(-1, 600).float()).reshape(-1, 3, 256, 256)\n",
    "    # # print(f\"IMU: {imu_x.shape}, flow: {flow.shape}\")\n",
    "    # flow = flow + imu_x\n",
    "    # flow = flow.detach().cpu().squeeze(0)\n",
    "    # print(flow.shape)\n",
    "    \n",
    "    img = img * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    mask = mask.numpy()\n",
    "    flow = flow.permute(1, 2, 0).numpy()\n",
    "    flow = flow * 255\n",
    "    img_painted = to_painted_image(img, mask)\n",
    "    # print image and flow\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(7.5, 5))\n",
    "    ax[0].imshow(img_painted)\n",
    "    ax[0].set_title(\"Image Annotated\")\n",
    "    ax[1].imshow(flow)\n",
    "    ax[1].set_title(\"Flow\")\n",
    "    plt.show()\n",
    "\n",
    "def view_masks(dataset, i):\n",
    "    img, flow, imu, mask, bbox = dataset[i]\n",
    "    img = img * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    # draw bounding boxes on img\n",
    "    img_bb = img.copy()\n",
    "    for b in bbox:\n",
    "        # to int\n",
    "        x1, y1, x2, y2 = [int(i) for i in b]\n",
    "        cv2.rectangle(img_bb, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    mask = mask.numpy()\n",
    "    # show image, mask[0] and mask[1]. Masks are binary, Show black and white\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(10, 5))\n",
    "    ax[0].imshow(img_bb)\n",
    "    ax[0].set_title(\"Imagem\")\n",
    "    ax[1].imshow(mask[0], cmap='gray')\n",
    "    ax[1].set_title(\"Em movimento\")\n",
    "    ax[2].imshow(mask[1], cmap='gray')\n",
    "    ax[2].set_title(\"Parado\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semseg.models.backbones import ResNet, PoolFormer, ConvNeXt\n",
    "from semseg.models.heads import UPerHead, LawinHead\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class SemSegModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.print_shape = True\n",
    "        self.backbone_flow = ResNet('18')\n",
    "        self.backbone_flow.load_state_dict(torch.load('../models/resnet18_a1.pth',\n",
    "                                                        map_location='cpu'), strict=False)\n",
    "        self.backbone_img = ResNet('50')\n",
    "        self.backbone_img.load_state_dict(torch.load('/home/thiago/Workspace/motion-segmentation/src/resnet50_a1.pth',\n",
    "                                                        map_location='cuda'), strict=False)\n",
    "        backbone_channels = self.backbone_img.channels + self.backbone_flow.channels\n",
    "        \n",
    "        print(backbone_channels)\n",
    "        self.head = LawinHead(backbone_channels, 128, num_classes=num_classes)\n",
    "    \n",
    "    def forward(self, img, flow, imu):\n",
    "        img_x = self.backbone_img(img)\n",
    "        flow_x = self.backbone_flow(flow)\n",
    "\n",
    "        if self.print_shape:\n",
    "            for i in img_x:\n",
    "                print(f\"Img shape: {i.shape}\")\n",
    "            for i in flow_x:\n",
    "                print(f\"Flow shape: {i.shape}\")\n",
    "            # for i in imu_x:\n",
    "            #     print(i.shape)\n",
    "\n",
    "        x = img_x + flow_x\n",
    "        \n",
    "        x = self.head(x)\n",
    "        \n",
    "        if self.print_shape:\n",
    "            print(f\"Output shape: {x.shape}\")\n",
    "            self.print_shape = False\n",
    "        \n",
    "        x = F.interpolate(x, size=(height, width), mode='bilinear', align_corners=False)\n",
    "        return x\n",
    "\n",
    "# model = SemSegModel().to(device)\n",
    "model = torch.load(\"models/last.pth\").to(device)\n",
    "model.eval()\n",
    "# model = torch.load(\"/home/thiago/Workspace/motion-segmentation/src/models/last_resnet.pth\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kitti_dir = \"/media/thiago/Data/Datasets/kitti/dataset/sequences/03\"\n",
    "generate_flow = True\n",
    "image_dir = os.path.join(kitti_dir, \"image_2\")\n",
    "flow_dir = os.path.join(kitti_dir, \"flow\")\n",
    "pred_dir = os.path.join(kitti_dir, \"pred\")\n",
    "if not os.path.exists(pred_dir):\n",
    "    os.makedirs(pred_dir)\n",
    "if not os.path.exists(flow_dir):\n",
    "    os.makedirs(flow_dir)\n",
    "images = sorted(os.listdir(image_dir))\n",
    "def get_next_image_name(filename):\n",
    "    # 000005.png -> 000010.png, always 6 digits\n",
    "    number = int(filename.split(\".\")[0])\n",
    "    number += 2\n",
    "    return f\"{number:06}.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute optical flow\n",
    "if generate_flow:\n",
    "    flow_model = ptlflow.get_model('dicl', pretrained_ckpt='kitti').cuda()\n",
    "    def flow_single_image(img_name):\n",
    "        try:\n",
    "            next_image_name = get_next_image_name(img_name)\n",
    "            path = os.path.join(image_dir, img_name)\n",
    "            next_image_path = os.path.join(image_dir, next_image_name)\n",
    "\n",
    "            flow_path = os.path.join(flow_dir, img_name)\n",
    "\n",
    "            # extract dir and create if not exists\n",
    "            # flow_dir = os.path.dirname(flow_path)\n",
    "            # recursively remove flow_dir\n",
    "            # if not os.path.exists(flow_dir):\n",
    "            #     os.makedirs(flow_dir, exist_ok=True)\n",
    "            # get next image name\n",
    "            # root_path is all before \"cam0\"\n",
    "            # compute flow\n",
    "            if next_image_path is None:\n",
    "                return\n",
    "            \n",
    "            prev_image = cv2.imread(path)\n",
    "            next_image = cv2.imread(next_image_path)\n",
    "\n",
    "            if prev_image is None or next_image is None:\n",
    "                return\n",
    "\n",
    "            imgs = [prev_image, next_image]\n",
    "            io_adapter = IOAdapter(flow_model, imgs[0].shape[:2])\n",
    "            inputs = io_adapter.prepare_inputs(imgs)\n",
    "            inputs['images'] = inputs['images'].cuda()\n",
    "\n",
    "            predictions = flow_model(inputs)\n",
    "            flows = predictions['flows']\n",
    "            flow_rgb = flow_utils.flow_to_rgb(flows, flow_max_radius=150)\n",
    "            # Make it a numpy array with HWC shape\n",
    "            flow_rgb = flow_rgb[0, 0].permute(1, 2, 0)\n",
    "            flow = flow_rgb.detach().cpu().numpy()\n",
    "            # OpenCV uses BGR format\n",
    "            flow = cv2.cvtColor(flow, cv2.COLOR_RGB2BGR)\n",
    "            # to 0-255\n",
    "            flow = (flow * 255).astype(np.uint8)\n",
    "            # return to original size\n",
    "            # flow = cv2.resize(flow, (width, height))\n",
    "            # save flow image\n",
    "            cv2.imwrite(flow_path, flow)\n",
    "            \n",
    "            \n",
    "            # free memory\n",
    "            del prev_image\n",
    "            del next_image\n",
    "            del flow\n",
    "            del flow_rgb\n",
    "            del flows\n",
    "            del predictions\n",
    "            del inputs\n",
    "            del imgs\n",
    "            gc.collect()\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {img_name} due to {e}\\n\")\n",
    "    # using pool, with tqdm\n",
    "    # with Pool(28) as p:\n",
    "    #     list(tq.tqdm(p.imap(flow_single_image, images), total=len(images)))\\\n",
    "\n",
    "    # without pool\n",
    "    for i, image in tq.tqdm(enumerate(images), total=len(images)):\n",
    "        flow_single_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = os.path.join(kitti_dir, \"results.avi\")\n",
    "# width, height = 512, 512\n",
    "img0 = Image.open(os.path.join(image_dir, images[0]))\n",
    "original_width, original_height = img0.size\n",
    "video = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*'XVID'), 15, (original_width, original_height))\n",
    "for i in tq.tqdm(range(len(images))):\n",
    "    img_path = os.path.join(image_dir, images[i])\n",
    "    img = Image.open(img_path)\n",
    "    img = transform(img)\n",
    "    img = normalize(img).unsqueeze(0).to(device)\n",
    "    flow_path = os.path.join(flow_dir, images[i])\n",
    "    if not os.path.exists(flow_path):\n",
    "        continue\n",
    "    flow = Image.open(flow_path)\n",
    "    flow = transform(flow)\n",
    "    flow = torch.div(flow, 255).unsqueeze(0).to(device)\n",
    "    imu = torch.zeros(2, 3, 100).to(device)\n",
    "    pred = model(img, flow, imu)\n",
    "\n",
    "    img = img.squeeze(0).cpu().detach()\n",
    "    img = img * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    img = img.numpy()\n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "    # label = label.squeeze(0).cpu().detach().numpy()\n",
    "    pred = pred.squeeze(0).cpu().detach().numpy()\n",
    "\n",
    "    pred_output = np.argmax(pred, axis=0)\n",
    "    pred_path = os.path.join(pred_dir, images[i])\n",
    "    # grey scale, class 0 is 255, class 1 is 128, class 2 is 0\n",
    "    pred_output[pred_output == 0] = 255\n",
    "    pred_output[pred_output == 1] = 128\n",
    "    pred_output[pred_output == 2] = 0\n",
    "    pred_output = pred_output.astype(np.uint8)\n",
    "    pred_output = cv2.resize(pred_output, (original_width, original_height))\n",
    "    cv2.imwrite(pred_path, pred_output)\n",
    "\n",
    "    painted_img = to_painted_image(img, pred)\n",
    "    painted_img = painted_img * 255\n",
    "    painted_img = painted_img.astype(np.uint8)\n",
    "    painted_img = cv2.cvtColor(painted_img, cv2.COLOR_RGB2BGR)\n",
    "    painted_img = cv2.resize(painted_img, (original_width, original_height))\n",
    "    video.write(painted_img)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
