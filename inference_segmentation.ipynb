{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import time\n",
    "import tqdm\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm.notebook as tq\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "from torch.nn import ConvTranspose2d\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import Module\n",
    "from torch.nn import ModuleList\n",
    "from torch.nn import ReLU\n",
    "from torchvision.transforms import CenterCrop\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import threading\n",
    "\n",
    "import json\n",
    "\n",
    "from semseg import show_models\n",
    "from semseg.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whuvid_dataset import WhuvidDataset\n",
    "from kitti_dataset import KittiDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width = 512, 512\n",
    "# height, width = 256, 256\n",
    "with_augs = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # transforms.RandomRotation(10),\n",
    "    transforms.RandomResizedCrop(size=(height, width), scale=(0.2, 1.0), ratio=(0.5, 2)),\n",
    "    # transforms.Resize((height, width)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((height, width)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whuvid_base_path = \"/home/thiago/Workspace/motion-segmentation/datasets/WHUVID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whuvid_train_dataset = WhuvidDataset(whuvid_base_path, [\"01\", \"02\", \"17\", \"19\", \"20\", \"23\", \"24\", \"25\", \"30\", \"31\", \"32\"], with_augs, segmentation=True, flow=True)\n",
    "val_dataset = WhuvidDataset(whuvid_base_path, [\"03\", \"18\", \"22\"], transform, segmentation=True, flow=True)\n",
    "train_dataset = whuvid_train_dataset\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_classes = 3\n",
    "\n",
    "def get_binary_masks(pred):\n",
    "    predicted_classes = np.argmax(pred, axis=0)\n",
    "    masks = []\n",
    "    for i in range(num_classes):\n",
    "        mask = (predicted_classes == i).astype(np.uint8)\n",
    "        masks.append(mask)\n",
    "    return masks\n",
    "\n",
    "def to_painted_image(image, label):\n",
    "    green_image = np.zeros_like(image)\n",
    "    blue_image = np.zeros_like(image)\n",
    "    green_image[:, :] = [0, 255, 0]  # Green color in BGR\n",
    "    blue_image[:, :] = [0, 0, 255]   # Blue color in BGR\n",
    "\n",
    "    masks = get_binary_masks(label)\n",
    "\n",
    "    alpha = 0.998  # Alpha value for blending\n",
    "    beta = 1.0 - alpha\n",
    "    blended_green = cv2.addWeighted(image, alpha, green_image, beta, 0)\n",
    "    blended_blue = cv2.addWeighted(image, alpha, blue_image, beta, 0)\n",
    "    blended_green = np.clip(blended_green, 0, 1)\n",
    "    blended_blue = np.clip(blended_blue, 0, 1)\n",
    "\n",
    "    output_image = image.copy()\n",
    "    output_image[masks[0] == 1] = blended_blue[masks[0] == 1]\n",
    "    output_image[masks[1] == 1] = blended_green[masks[1] == 1]\n",
    "\n",
    "    return output_image\n",
    "\n",
    "def print_dataset(dataset, i):\n",
    "    img, flow, imu, mask = dataset[i]\n",
    "    \n",
    "    img = img * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    mask = mask.numpy()\n",
    "    flow = flow.permute(1, 2, 0).numpy()\n",
    "    flow = flow * 255\n",
    "    img_painted = to_painted_image(img, mask)\n",
    "    # print image and flow\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(7.5, 5))\n",
    "    ax[0].imshow(img_painted)\n",
    "    ax[0].set_title(\"Image Annotated\")\n",
    "    ax[1].imshow(flow)\n",
    "    ax[1].set_title(\"Flow\")\n",
    "    plt.show()\n",
    "\n",
    "def view_masks(dataset, i):\n",
    "    img, flow, imu, mask, bbox = dataset[i]\n",
    "    img = img * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    # draw bounding boxes on img\n",
    "    img_bb = img.copy()\n",
    "    for b in bbox:\n",
    "        # to int\n",
    "        x1, y1, x2, y2 = [int(i) for i in b]\n",
    "        cv2.rectangle(img_bb, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    mask = mask.numpy()\n",
    "    # show image, mask[0] and mask[1]. Masks are binary, Show black and white\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(10, 5))\n",
    "    ax[0].imshow(img_bb)\n",
    "    ax[0].set_title(\"Imagem\")\n",
    "    ax[1].imshow(mask[0], cmap='gray')\n",
    "    ax[1].set_title(\"Em movimento\")\n",
    "    ax[2].imshow(mask[1], cmap='gray')\n",
    "    ax[2].set_title(\"Parado\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = random.randint(0, len(train_dataset)-1)\n",
    "print_dataset(train_dataset, random_index)\n",
    "height, width = train_dataset[0][0].shape[1:]\n",
    "\n",
    "random_index = random.randint(0, len(val_dataset)-1)\n",
    "print_dataset(val_dataset, random_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =  10\n",
    "data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=12, drop_last=True)\n",
    "test_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=24, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semseg.models.backbones import ResNet, PoolFormer, ConvNeXt\n",
    "from semseg.models.heads import UPerHead, LawinHead\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class SemSegModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone_flow = ResNet('18')\n",
    "        self.backbone_flow.load_state_dict(torch.load('../models/resnet18_a1.pth',\n",
    "                                                        map_location='cpu'), strict=False)\n",
    "        self.backbone_img = ResNet('50')\n",
    "        self.backbone_img.load_state_dict(torch.load('/home/thiago/Workspace/motion-segmentation/src/resnet50_a1.pth',\n",
    "                                                        map_location='cuda'), strict=False)\n",
    "        \n",
    "        backbone_channels = self.backbone_img.channels + self.backbone_flow.channels\n",
    "        \n",
    "        print(backbone_channels)\n",
    "        self.head = LawinHead(backbone_channels, 128, num_classes=num_classes)\n",
    "    \n",
    "    def forward(self, img, flow, imu):\n",
    "        \n",
    "        img_x = self.backbone_img(img)\n",
    "        flow_x = self.backbone_flow(flow)\n",
    "        \n",
    "        \n",
    "        x = img_x + flow_x\n",
    "        \n",
    "        \n",
    "        x = self.head(x)\n",
    "        \n",
    "        x = F.interpolate(x, size=(height, width), mode='bilinear', align_corners=False)\n",
    "        return x\n",
    "\n",
    "model = SemSegModel().to(device)\n",
    "model = torch.load(\"/home/thiago/Workspace/motion-segmentation/src/models/last-512-2.pth\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dataset(train_dataset, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_score(output, target):\n",
    "    output = torch.where(output > 0.5, 1, 0)\n",
    "    target = torch.where(target > 0.5, 1, 0)\n",
    "\n",
    "    smooth = 1e-6\n",
    "\n",
    "    if torch.is_tensor(output):\n",
    "        output = torch.sigmoid(output)\n",
    "\n",
    "    output = torch.round(output)\n",
    "\n",
    "    intersection = (output * target).sum()\n",
    "    union = (output + target).sum() - intersection\n",
    "\n",
    "    if target.sum() == 0:\n",
    "        return (output.sum() == 0).float()\n",
    "\n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_3 = \"22\"\n",
    "seq_3_dataset = WhuvidDataset(whuvid_base_path, [seq_3], transform, segmentation=True, flow=True, use_gdino=False)\n",
    "print(len(seq_3_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "def get_results(video_dataset, i):\n",
    "    image, flow, imu, label = video_dataset[i]\n",
    "\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    flow =  flow.unsqueeze(0).to(device)\n",
    "    imu = imu.unsqueeze(0).to(device)\n",
    "    label = label.unsqueeze(0).to(device)\n",
    "    \n",
    "    pred = model(image, flow, imu)\n",
    "    for i in range(3):\n",
    "        iou = iou_score(pred[:,i], label[:,i]).item()\n",
    "        # print(f\"IoU {i}: {iou}\")\n",
    "\n",
    "    image = image.squeeze(0).cpu().detach()\n",
    "    image = image * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    image = image.numpy()\n",
    "    image = np.transpose(image, (1, 2, 0))\n",
    "    flow = flow.squeeze(0).cpu().detach().numpy()\n",
    "    flow = np.transpose(flow, (1, 2, 0))\n",
    "    flow = flow * 255\n",
    "    imu = imu.squeeze(0).cpu().detach().numpy()\n",
    "    label = label.squeeze(0).cpu().detach().numpy()\n",
    "    pred = pred.squeeze(0).cpu().detach().numpy()\n",
    "\n",
    "    output_image = to_painted_image(image, label)\n",
    "\n",
    "    output_image_pred = to_painted_image(image, pred)\n",
    "\n",
    "    return output_image_pred, output_image, flow, pred\n",
    "    \n",
    "\n",
    "def plot_results(dataset, i):\n",
    "    output_image_pred, output_image, flow, pred = get_results(dataset, i)\n",
    "\n",
    "    # plot 4 images: output_image_pred, output_image and flow_vis\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(30, 10))\n",
    "    for axis in axes:\n",
    "        axis.set_axis_off()\n",
    "    output_image_pred = cv2.resize(output_image_pred, (1280, 720), interpolation=cv2.INTER_NEAREST)\n",
    "    axes[0].imshow(output_image_pred)\n",
    "    axes[0].set_title(\"Predição\")\n",
    "    output_image = cv2.resize(output_image, (1280, 720), interpolation=cv2.INTER_NEAREST)\n",
    "    axes[1].imshow(output_image)\n",
    "    axes[1].set_title(\"Anotação\")\n",
    "    # flow = cv2.resize(flow, (1280, 720), interpolation=cv2.INTER_NEAREST)\n",
    "    # axes[2].imshow(flow)\n",
    "\n",
    "\n",
    "i = 765\n",
    "# i = 440\n",
    "# print(val_dataset.images[i])\n",
    "# i = index_by_loss[1400]\n",
    "# print(f\"iou[0]: {results[i][0]}, iou[1]: {results[i][1]}, loss: {losses[i]}\")\n",
    "plot_results(seq_3_dataset, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_sequence = \"18\" # 03, 18, 22\n",
    "video_dataset = WhuvidDataset(whuvid_base_path, [video_sequence], transform, segmentation=True, flow=True, use_gdino=False)\n",
    "video_path = f\"{whuvid_base_path}/{video_sequence}/other_files/results.avi\"\n",
    "pred_path = f\"{whuvid_base_path}/{video_sequence}/pred\"\n",
    "if not os.path.exists(pred_path):\n",
    "    os.mkdir(pred_path)\n",
    "img0, _, _, _ = video_dataset[0]\n",
    "width, height = img0.shape[1:]\n",
    "video = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*'XVID'), 30, (width, height))\n",
    "model.eval()\n",
    "for i in tq.tqdm(range(len(video_dataset))):\n",
    "    img_path = video_dataset.images[i]\n",
    "    basename = os.path.basename(img_path)\n",
    "    output_path = os.path.join(pred_path, basename)\n",
    "    output_image_pred, _, _, pred_output = get_results(video_dataset, i)\n",
    "\n",
    "    output_image_pred = output_image_pred * 255\n",
    "    output_image_pred = output_image_pred.astype(np.uint8)\n",
    "    output_image_pred = cv2.cvtColor(output_image_pred, cv2.COLOR_RGB2BGR)\n",
    "    video.write(output_image_pred)\n",
    "\n",
    "    pred_output = np.argmax(pred_output, axis=0)\n",
    "    pred_output_view = pred_output\n",
    "    pred_output[pred_output == 0] = 255\n",
    "    # pred_output[pred_output == 1] = 128\n",
    "    pred_output[pred_output == 1] = 0\n",
    "    pred_output[pred_output == 2] = 0\n",
    "    pred_output = pred_output.astype(np.uint8)\n",
    "    pred_output = cv2.dilate(pred_output, np.ones((2, 2), np.uint8), iterations=1)\n",
    "    pred_output = cv2.resize(pred_output, (1280, 720), interpolation=cv2.INTER_NEAREST)\n",
    "    # save pred\n",
    "    cv2.imwrite(output_path, pred_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
