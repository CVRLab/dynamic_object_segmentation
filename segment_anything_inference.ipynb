{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "from whuvid_dataset import WhuvidDataset\n",
    "import torchvision.transforms as transforms\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "    \n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "    \n",
    "def show_box(box, ax, color='green'):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor=color, facecolor=(0,0,0,0), lw=2)) \n",
    "\n",
    "# torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "# sam_checkpoint = \"/home/thiago/Workspace/motion-segmentation/models/sam_vit_h_4b8939.pth\"\n",
    "# model_type = \"vit_h\"\n",
    "# sam_checkpoint = \"/home/thiago/Workspace/motion-segmentation/models/sam_vit_l_0b3195.pth\"\n",
    "# model_type = \"vit_l\"\n",
    "sam_checkpoint = \"/home/thiago/Workspace/motion-segmentation/models/sam_vit_b_01ec64.pth\"\n",
    "model_type = \"vit_b\"\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences = [\"01\", \"17\", \"18\", \"19\", \"20\", \"22\", \"25\", \"30\", \"31\", \"32\"]\n",
    "sequences = [\"03\"]\n",
    "# 16 missing\n",
    "# sequence = sequences[0]\n",
    "root_path = \"/home/thiago/Workspace/motion-segmentation/datasets/WHUVID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# width = int(1280 * 0.5)\n",
    "# height = int(720 * 0.5)\n",
    "width = 1280\n",
    "height = 720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_list_of_boxes(bb_raw):\n",
    "    global width, height\n",
    "    bb_of_cars = [x['relative_coordinates'] for x in bb_raw if x['name'] == \"car\"]\n",
    "    # to pixels\n",
    "    bb_of_cars_pixel = [{'center_x': x['center_x'] * width, 'center_y': x['center_y'] * height,\n",
    "                        'width': x['width'] * width, 'height': x['height'] * height} for x in bb_of_cars]\n",
    "    boxes = []\n",
    "    for bb in bb_of_cars_pixel:\n",
    "        x0, y0 = bb['center_x'] - bb['width'] / 2, bb['center_y'] - bb['height'] / 2\n",
    "        x1, y1 = x0 + bb['width'], y0 + bb['height']\n",
    "        boxes.append([x0, y0, x1, y1])\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(sequence):\n",
    "    whuvid_base_path = \"/home/thiago/Workspace/motion-segmentation/datasets/WHUVID\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    whuvid_dataset = WhuvidDataset(whuvid_base_path, [sequence], transform, segmentation=False, flow=False)\n",
    "    # cam_path = os.path.join(path, \"cam0\")\n",
    "    # images = []\n",
    "    objects_inferred_path = f\"/home/thiago/Workspace/motion-segmentation/datasets/WHUVID/{sequence}/other_files/objects_inferred.json\"\n",
    "    with open(objects_inferred_path) as f:\n",
    "        objects_inferred = json.load(f)\n",
    "    bounding_boxes = {}\n",
    "    for obj_id, frames in objects_inferred.items():\n",
    "        for frame_id, ann in frames.items():\n",
    "            frame_id = int(frame_id)\n",
    "            if frame_id not in bounding_boxes:\n",
    "                bounding_boxes[frame_id] = []\n",
    "            bounding_boxes[frame_id].append(ann)\n",
    "    return whuvid_dataset.images, bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_mask(image, boxes):\n",
    "    if len(boxes) == 0:\n",
    "        return torch.zeros(1, image.shape[0], image.shape[1], device=predictor.device)\n",
    "    input_boxes = torch.tensor(boxes, device=predictor.device)\n",
    "    transformed_boxes = predictor.transform.apply_boxes_torch(input_boxes, image.shape[:2])\n",
    "    predictor.set_image(image)\n",
    "    with torch.no_grad():\n",
    "        masks, _, _ = predictor.predict_torch(\n",
    "            point_coords=None,\n",
    "            point_labels=None,\n",
    "            boxes=transformed_boxes,\n",
    "            multimask_output=False,\n",
    "        )\n",
    "    # mask has shape (n, 1, h, w), where n is the number of masks\n",
    "    # values are boolean\n",
    "    # change to (1, h, w) by joining all masks\n",
    "    mask = masks.sum(dim=0, keepdim=True)\n",
    "    # limit to 0 or 1\n",
    "    mask = mask.clamp(0, 1)\n",
    "    return mask\n",
    "\n",
    "def to_mask(img_path, boxes_and_labels):\n",
    "    image = cv2.imread(img_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    width, height = image.shape[1], image.shape[0]\n",
    "    image = cv2.resize(image, (width, height))\n",
    "    boxes1 = [x[0] for x in boxes_and_labels if x[1] == True]\n",
    "    boxes2 = [x[0] for x in boxes_and_labels if x[1] == False]\n",
    "    # set any value lower than 0 to 0\n",
    "    boxes1 = [[max(0, x) for x in box] for box in boxes1]\n",
    "    boxes2 = [[max(0, x) for x in box] for box in boxes2]\n",
    "    # labels = [x[1] for x in boxes_and_labels]\n",
    "    mask1 = gen_mask(image, boxes1)\n",
    "    mask2 = gen_mask(image, boxes2)\n",
    "    # if they overlap, the last one will be shown\n",
    "    mask = 255 * mask1 + 128 * mask2\n",
    "    # clamp to 0-255\n",
    "    mask = mask.clamp(0, 255)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift bounding boxes in pixels\n",
    "def shift_bb(bb, horizontal, vertical):\n",
    "    return [[x[0] + horizontal, x[1] + vertical, x[2] + horizontal, x[3] + vertical] for x in bb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increase_bb(bb, horizontal, vertical):\n",
    "    horizontal = int(horizontal)\n",
    "    vertical = int(vertical)\n",
    "    increased = [[x[0] - horizontal, x[1] - vertical, x[2] + horizontal, x[3] + vertical] for x in bb]\n",
    "    # limit to image size\n",
    "    increased = [[max(0, x[0]), max(0, x[1]), min(width, x[2]), min(height, x[3])] for x in increased]\n",
    "    return increased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 30\n",
    "seq_example = \"22\"\n",
    "# example_path = os.path.join(root_path, seq_example)\n",
    "images, bounding_boxes = get_data(seq_example)\n",
    "img_path = images[i]\n",
    "# boxes = bounding_boxes[i]\n",
    "boxes = bounding_boxes[i]\n",
    "percent = 0.01\n",
    "# boxes = increase_bb(bounding_boxes[i], percent * width, percent * height)\n",
    "# boxes = increase_bb(bounding_boxes[i], 8, 8)\n",
    "# boxes = shift_bb(boxes, 10, -20)\n",
    "image = cv2.imread(img_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image = cv2.resize(image, (width, height))\n",
    "mask = to_mask(img_path, boxes)\n",
    "# in tensor\n",
    "label1 = torch.tensor(mask >= 255, dtype=torch.uint8)\n",
    "# lower than 255 and higher than 128\n",
    "label2 = torch.tensor((mask < 255) & (mask >= 128), dtype=torch.uint8)\n",
    "# show_mask(mask.cpu().numpy(), plt.gca())\n",
    "# input_boxes = torch.tensor(boxes, device=predictor.device)\n",
    "image_bb = image.copy()\n",
    "for item in boxes:\n",
    "    box = torch.tensor(item[0], device=predictor.device)\n",
    "    is_moving = item[1]\n",
    "    # use cv\n",
    "    x0, y0, x1, y1 = box.cpu().numpy().astype(int)\n",
    "    color = (0, 0, 255) if is_moving else (0, 255, 0)\n",
    "    cv2.rectangle(image_bb, (x0, y0), (x1, y1), color, 3)\n",
    "# show image_bb, label1, label2\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 10))\n",
    "# add legend: Image, Moving and Static\n",
    "ax[0].set_title(\"Imagem com bounding box\")\n",
    "ax[1].set_title(\"Mascara de objetos em movimento\")\n",
    "ax[2].set_title(\"Mascara de objetos estaticos\")\n",
    "ax[0].imshow(image_bb)\n",
    "ax[1].imshow(label1.cpu().numpy().squeeze(), cmap='gray')\n",
    "ax[2].imshow(label2.cpu().numpy().squeeze(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path = os.path.join(root_path, '22')\n",
    "cam_path = os.path.join(path, \"cam0\")\n",
    "images, bounding_boxes = get_data(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bounding_boxes[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq in tqdm.tqdm(sequences):\n",
    "    path = os.path.join(root_path, seq)\n",
    "    cam_path = os.path.join(path, \"cam0\")\n",
    "    mask_dir = cam_path + \"_masks_ann\"\n",
    "    if not os.path.exists(mask_dir):\n",
    "        os.makedirs(mask_dir)\n",
    "    images, bounding_boxes = get_data(seq)\n",
    "    # generate masks for every image and save them\n",
    "    # get in batches of 2\n",
    "    for i in tqdm.tqdm(range(len(images))):\n",
    "        basename = os.path.basename(images[i])\n",
    "        mask_path = os.path.join(mask_dir, basename)\n",
    "        # if os.path.exists(mask_path):\n",
    "        #     continue\n",
    "        if i in bounding_boxes:\n",
    "            try:\n",
    "                mask = to_mask(images[i], bounding_boxes[i])\n",
    "                # mask = to_mask(images[i], shift_bb(bounding_boxes[i], 10, -20))\n",
    "            except Exception as e:\n",
    "                print(\"Skipping image \", images[i], \" because of error:\", e)\n",
    "                continue\n",
    "            mask = mask.cpu().numpy()\n",
    "            mask = mask.astype(np.uint8)\n",
    "            # mask = mask * 255\n",
    "            mask = mask.squeeze()\n",
    "        else:\n",
    "            mask = np.zeros((height, width), dtype=np.uint8)\n",
    "        cv2.imwrite(mask_path, mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
